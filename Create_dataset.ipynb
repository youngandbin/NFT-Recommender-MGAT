{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Select NFT collection name from ['azuki', 'bayc', 'coolcats', 'doodles', 'meebits']\n",
    "'''\n",
    "\n",
    "COLLECTION = 'azuki'\n",
    "df_azuki = pd.read_csv(f\"dataset/transactions/{COLLECTION}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data preprocessing (filtering)\n",
    "\"\"\"\n",
    "\n",
    "# 1) drop duplicated interactions (i.e., drop rows that Buyer and Token ID are identical)\n",
    "df_azuki = df_azuki.drop_duplicates(subset=['Buyer', 'Token ID'], keep='first')\n",
    "\n",
    "# 2) Exclude items that we do not have features data for.\n",
    "#   That is, Only items that exist in the item features file will be left.\n",
    "#   For reference, all items existing in the transaction data were first traded before September 2022.\n",
    "image = pd.read_csv(f'dataset/item_features/{COLLECTION}_image.csv', index_col=0)\n",
    "text = pd.read_csv(f'dataset/item_features/{COLLECTION}_text.csv', index_col=0)\n",
    "price = pd.read_csv(f'dataset/item_features/{COLLECTION}_prices.csv', index_col=0)\n",
    "transaction = pd.read_csv(f'dataset/item_features/{COLLECTION}_transactions.csv', index_col=0)\n",
    "df_azuki = df_azuki[df_azuki['Token ID'].isin(image.index)]\n",
    "df_azuki = df_azuki[df_azuki['Token ID'].isin(text.index)]\n",
    "df_azuki = df_azuki[df_azuki['Token ID'].isin(price.index)]\n",
    "df_azuki = df_azuki[df_azuki['Token ID'].isin(transaction.index)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create interactions ('inter')\n",
    "- input\n",
    "    - NFT transactions data in 'transactions' folder, collected from Etherscan NFT tracker (https://etherscan.io/nfttracker)\n",
    "- output\n",
    "    - An .npy formatted interaction file (user, item, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set save_path if not exist\n",
    "save_path = 'dataset/collections/'+COLLECTION+'/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# ITEM_CUT: Only items that have been traded CUT times or more will be used.\n",
    "# \"\"\"\n",
    "# CUT = 5\n",
    "\n",
    "# # print len of df_azuki\n",
    "# print(f\"ITEM CUT 전 거래 개수: {len(df_azuki)}\")\n",
    "\n",
    "# # get the list of \"Token ID\" whose count is more than 3\n",
    "# item_count = df_azuki['Token ID'].value_counts()\n",
    "# item_count = item_count[item_count >= CUT]\n",
    "# item_count = item_count.index.tolist()\n",
    "\n",
    "# # drop rows whose \"Token ID\" is not in item_count\n",
    "# df_azuki = df_azuki[df_azuki['Token ID'].isin(item_count)]\n",
    "\n",
    "# # print len of df_azuki\n",
    "# print(f\"ITEM CUT 후 거래 개수: {len(df_azuki)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# USER_CUT: Only users that have been traded CUT times or more will be used.\n",
    "# \"\"\"\n",
    "# CUT = 5\n",
    "\n",
    "# # print len of df_azuki\n",
    "# print(f\"USER CUT 전 거래 개수: {len(df_azuki)}\")\n",
    "\n",
    "# # get the list of \"Buyer\" whose count is more than 3\n",
    "# user_count = df_azuki['Buyer'].value_counts()\n",
    "# user_count = user_count[user_count >= CUT]\n",
    "# user_count = user_count.index.tolist()\n",
    "\n",
    "# # drop rows whose \"Buyer\" is not in user_count\n",
    "# df_azuki = df_azuki[df_azuki['Buyer'].isin(user_count)]\n",
    "\n",
    "# # print len of df_azuki\n",
    "# print(f\"USER CUT 후 거래 개수: {len(df_azuki)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate price labels, for later use of multi-objectives training\n",
    "\"\"\"\n",
    "\n",
    "# drop rows where 'Price' does not contain '$'\n",
    "df_azuki = df_azuki[df_azuki['Price'].str.contains(\"\\$\")]\n",
    "# convert 'Price' to the value before 'ETH'\n",
    "df_azuki['Price'] = df_azuki['Price'].apply(lambda x: x.split(' ')[2][2:-1].replace(',', '').replace('.', ''))\n",
    "df_azuki['Price'] = df_azuki['Price'].astype(float)\n",
    "\n",
    "# create a new variable 'Price_diff' which is the difference between the future price and the current price \n",
    "# get price differences from the same 'Token ID'\n",
    "df_azuki['Price_diff'] = df_azuki.groupby('Token ID')['Price'].diff(-1)\n",
    "# convert rows where 'Price_diff' is NaN into 0\n",
    "df_azuki['Price_diff'] = df_azuki['Price_diff'].fillna(0)\n",
    "# put minus to Price_diff\n",
    "df_azuki['Price_diff'] = df_azuki['Price_diff'].apply(lambda x: -x)\n",
    "# convert 'Price_diff' to 1 if the value is greater than 0, otherwise 0\n",
    "df_azuki['Price_diff'] = df_azuki['Price_diff'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# create an np.array with 'Buyer'\n",
    "user = df_azuki['Buyer'].values\n",
    "item = df_azuki['Token ID'].values\n",
    "labels = df_azuki['Price_diff'].values\n",
    "\n",
    "data = (user, item, labels)\n",
    "\n",
    "# save and read npy file\n",
    "np.save(save_path + f'{COLLECTION}.npy', data)\n",
    "azuki = np.load(save_path + COLLECTION+'.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user length:  11811\n",
      "item length:  8386\n",
      "inter length:  22987\n"
     ]
    }
   ],
   "source": [
    "user = azuki[0]\n",
    "item = azuki[1]\n",
    "labels = azuki[2]\n",
    "\n",
    "# print user length and item length\n",
    "print('user length: ', len(set(user)))\n",
    "print('item length: ', len(set(item)))\n",
    "print('inter length: ', len(labels))\n",
    "\n",
    "# save user length and item length as a dictionary\n",
    "dict = {'num_user': len(set(user)), 'num_item': len(set(item))}\n",
    "np.save(save_path + 'num_user_item.npy', dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5457, 1430,    0],\n",
       "       [3360,  845,    1],\n",
       "       [7595, 1431,    1],\n",
       "       ...,\n",
       "       [7166, 6322,    0],\n",
       "       [1987, 3792,    0],\n",
       "       [9862, 9251,    0]], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Change the user addresses to integers starting from 0.\n",
    "e.g., 0x9137a5d195f0ab57e428c5a2be9bc8c4620445cb -> 0\n",
    "\"\"\"\n",
    "\n",
    "# create a dict where keys are user and values are new indices starting from 0\n",
    "user_unique = np.unique(user)\n",
    "mapping_u = {}\n",
    "for i in range(len(user_unique)):\n",
    "    mapping_u[user_unique[i]] = i\n",
    "\n",
    "# apply mapping to user\n",
    "user = np.array([mapping_u[u] for u in user])\n",
    "\n",
    "# create a 2D np.array where first columns are users and second column is items\n",
    "inter = np.array([user, item, labels]).T\n",
    "# convert inter type as int64\n",
    "inter = inter.astype(np.int64)\n",
    "inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TEMPORAL\n",
    "TEMPORAL\n",
    "TEMPORAL\n",
    "TEMPORAL\n",
    "TEMPORAL\n",
    "\n",
    "Split the data into train, validation, and test sets.\n",
    "And get indices of train, validation, and test sets.\n",
    "\"\"\"\n",
    "\n",
    "inter_len = len(inter)\n",
    "# get random indices\n",
    "indices = np.random.permutation(inter_len)\n",
    "# split indices into train, validation, and test sets\n",
    "train_indices = indices[:int(inter_len*0.8)]\n",
    "val_indices = indices[int(inter_len*0.8):int(inter_len*0.9)]\n",
    "test_indices = indices[int(inter_len*0.9):]\n",
    "\n",
    "# create a list of lists, where each list contains indices of train, validation, and test sets\n",
    "indices = [list(train_indices), list(val_indices), list(test_indices)]\n",
    "\n",
    "# save indices as pkl file\n",
    "import pickle\n",
    "with open(save_path + 'indices.pkl', 'wb') as f:\n",
    "    pickle.dump(indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13843,  1080,     0],\n",
       "       [11746,   576,     1],\n",
       "       [15981,  1081,     1],\n",
       "       ...,\n",
       "       [15552,  5292,     0],\n",
       "       [10373,  3121,     0],\n",
       "       [18248,  7837,     0]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensure that the indices for the user and item do not overlap with each other.\n",
    "We add len(set(item)) to the user indices.\n",
    "We map the item indices to the range of [0, len(set(item))).\n",
    "\n",
    "For example,\n",
    "    Before:\n",
    "        user: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        item: [0, 3, 4, 7, 8, 9, 10, 20, 21, 22]\n",
    "    After:\n",
    "        user: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "        item: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 1) Change the user idx: start from num_item.\n",
    "\n",
    "num_item = len(set(item))\n",
    "user += num_item\n",
    "\n",
    "inter = np.array([user, item, labels]).T\n",
    "inter = inter.astype(np.int64)\n",
    "\n",
    "\n",
    "# 2) Map the item idx: start from 0.\n",
    "\n",
    "# create a dict where keys are item and values are new indices starting from 0\n",
    "item_unique = np.unique(item)\n",
    "mapping_i = {}\n",
    "for i in range(len(item_unique)):\n",
    "    mapping_i[item_unique[i]] = i\n",
    "mapping_i\n",
    "\n",
    "# convert the second column of inter to new indices using mapping\n",
    "inter[:, 1] = [mapping_i[i] for i in inter[:, 1]]\n",
    "inter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create user features (user_feat.npy)\n",
    "- input\n",
    "    - User features data in 'user_features' folder, collected and preprocessed from transactions file\n",
    "- output\n",
    "    - An .npy formatted user features file ('# of transactions', 'Avg transaction price', 'avg holding period')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mapping_u' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m df_feature[names] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(df_feature[names])\n\u001b[0;32m      9\u001b[0m \u001b[39m# convert column 'Buyer' using mapping_u\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# if the value is not in mapping_u, remove the row\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df_feature[\u001b[39m'\u001b[39m\u001b[39mBuyer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_feature[\u001b[39m'\u001b[39;49m\u001b[39mBuyer\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: mapping_u[x] \u001b[39mif\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m mapping_u \u001b[39melse\u001b[39;49;00m np\u001b[39m.\u001b[39;49mnan)\n\u001b[0;32m     12\u001b[0m df_feature \u001b[39m=\u001b[39m df_feature\u001b[39m.\u001b[39mdropna()\n\u001b[0;32m     13\u001b[0m \u001b[39m# convert column 'Buyer' to int\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lesga\\anaconda3\\envs\\RecBole\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\lesga\\anaconda3\\envs\\RecBole\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\lesga\\anaconda3\\envs\\RecBole\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\lesga\\anaconda3\\envs\\RecBole\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      7\u001b[0m df_feature[names] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(df_feature[names])\n\u001b[0;32m      9\u001b[0m \u001b[39m# convert column 'Buyer' using mapping_u\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# if the value is not in mapping_u, remove the row\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df_feature[\u001b[39m'\u001b[39m\u001b[39mBuyer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_feature[\u001b[39m'\u001b[39m\u001b[39mBuyer\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: mapping_u[x] \u001b[39mif\u001b[39;00m x \u001b[39min\u001b[39;00m mapping_u \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mnan)\n\u001b[0;32m     12\u001b[0m df_feature \u001b[39m=\u001b[39m df_feature\u001b[39m.\u001b[39mdropna()\n\u001b[0;32m     13\u001b[0m \u001b[39m# convert column 'Buyer' to int\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mapping_u' is not defined"
     ]
    }
   ],
   "source": [
    "# read 'user features.csv'\n",
    "df_feature = pd.read_csv('dataset/user_features/user_features.csv', index_col=0).drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "# scaling columns \"# of transactions\", \"Avg transaction price\", \"avg holding period\": MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "names = ['# of transactions', 'Avg transaction price', 'avg holding period']\n",
    "df_feature[names] = scaler.fit_transform(df_feature[names])\n",
    "\n",
    "# convert column 'Buyer' using mapping_u\n",
    "# if the value is not in mapping_u, remove the row\n",
    "df_feature['Buyer'] = df_feature['Buyer'].apply(lambda x: mapping_u[x] if x in mapping_u else np.nan)\n",
    "df_feature = df_feature.dropna()\n",
    "# convert column 'Buyer' to int\n",
    "df_feature['Buyer'] = df_feature['Buyer'].astype(int)\n",
    "print('num_user: ', len(df_feature))\n",
    "\n",
    "# set 'Buyer' as index\n",
    "df_feature = df_feature.set_index('Buyer')\n",
    "\n",
    "# save df as npy file\n",
    "np.save(save_path+'user_feat.npy', df_feature, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train data (train.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (18389, 3)\n",
      "valid shape:  (2299, 3)\n",
      "test shape:  (2299, 3)\n"
     ]
    }
   ],
   "source": [
    "# random split inter\n",
    "train, valid_and_test = train_test_split(inter, test_size=0.2, random_state=2023)\n",
    "valid, test = train_test_split(valid_and_test, test_size=0.5, random_state=2023)\n",
    "\n",
    "# print train, valid shape\n",
    "print('train shape: ', train.shape)\n",
    "print('valid shape: ', valid.shape)\n",
    "print('test shape: ', test.shape)\n",
    "\n",
    "# save inter as npy file\n",
    "np.save(save_path+'train.npy', train, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create valid data (val.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15556, [1608]),\n",
       " (10417, [7924, 2246]),\n",
       " (11908, [8218]),\n",
       " (20172, [2021]),\n",
       " (15446, [5431, 453, 5260, 4947, 5978, 5242])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using valid, create a dict where keys are unique users and values are items\n",
    "\n",
    "valid_dict = {}\n",
    "for i in range(len(valid)):\n",
    "    if valid[i][0] in valid_dict:\n",
    "        valid_dict[valid[i][0]].append(valid[i][1])\n",
    "    else:\n",
    "        valid_dict[valid[i][0]] = [valid[i][1]]\n",
    "\n",
    "# show the first five items in valid_dict\n",
    "list(valid_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the item index in the order of the most traded (popular).\n",
    "\"\"\"\n",
    "\n",
    "# concat all values in valid_dict as a list\n",
    "valid_list = []\n",
    "for i in valid_dict.values():\n",
    "    valid_list += i\n",
    "\n",
    "# value count valid_list and sort values\n",
    "value_counts = pd.Series(valid_list).value_counts().sort_values(ascending=False)\n",
    "\n",
    "# extract indices of value_counts\n",
    "indices = value_counts.index\n",
    "\n",
    "# save indices as npy\n",
    "np.save(save_path+'indices_valid.npy', indices, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([15556, 1608]), list([10417, 7924, 2246]),\n",
       "       list([11908, 8218]), list([20172, 2021]),\n",
       "       list([15446, 5431, 453, 5260, 4947, 5978, 5242])], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert to the form required by the model\n",
    "e.g., 12656: [7314, 4820, 6304] -> list([12656, 7314, 4820, 6304])\n",
    "\"\"\"\n",
    "\n",
    "# Create an empty numpy array with dtype 'object'\n",
    "my_array = np.empty(len(valid_dict), dtype=object)\n",
    "\n",
    "# Assign the lists directly to the elements of the array\n",
    "for i, (key, val) in enumerate(valid_dict.items()):\n",
    "    # include key in the list\n",
    "    my_array[i] = [key] + val\n",
    "\n",
    "# show the first five items in my_array\n",
    "my_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save my_array as npy file\n",
    "\n",
    "np.save(save_path+'val.npy', my_array, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test data (test.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10739, [7712]),\n",
       " (12897, [1753]),\n",
       " (13508, [1484, 6924]),\n",
       " (19022, [2484]),\n",
       " (14053, [4533])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using test, create a dict where keys are unique users and values are items\n",
    "\n",
    "test_dict = {}\n",
    "for i in range(len(test)):\n",
    "    if test[i][0] in test_dict:\n",
    "        test_dict[test[i][0]].append(test[i][1])\n",
    "    else:\n",
    "        test_dict[test[i][0]] = [test[i][1]]\n",
    "\n",
    "# show the first five items in test_dict\n",
    "list(test_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all values in test_dict as a list\n",
    "\n",
    "test_list = []\n",
    "for i in test_dict.values():\n",
    "    test_list += i\n",
    "\n",
    "\n",
    "# value count test_list and sort values\n",
    "\n",
    "value_counts = pd.Series(test_list).value_counts().sort_values(ascending=False)\n",
    "\n",
    "# extract indices of value_counts\n",
    "\n",
    "indices = value_counts.index\n",
    "indices\n",
    "\n",
    "# save indices as npy\n",
    "\n",
    "np.save(save_path+'indices_test.npy', indices, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([10739, 7712]), list([12897, 1753]),\n",
       "       list([13508, 1484, 6924]), list([19022, 2484]),\n",
       "       list([14053, 4533])], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty numpy array with dtype 'object'\n",
    "my_array = np.empty(len(test_dict), dtype=object)\n",
    "\n",
    "# Assign the lists directly to the elements of the array\n",
    "for i, (key, val) in enumerate(test_dict.items()):\n",
    "    # include key in the list\n",
    "    my_array[i] = [key] + val\n",
    "\n",
    "# show the first five items in my_array\n",
    "my_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래 이렇게하면 됐었는데 에러나서 버림\n",
    "\n",
    "# # convert test_dict into a 1D np.array where each element is a list\n",
    "# # a list where the first element is the key of test_dict and the value is the value of test_dict\n",
    "\n",
    "# test_dict = np.array([[k]+v for k, v in test_dict.items()])\n",
    "# test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test_dict as npy file\n",
    "\n",
    "np.save(save_path+'test.npy', my_array, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create adjacency matrix (adj_dict.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13843, [1080, 5354, 1189]),\n",
       " (11746, [576, 5046]),\n",
       " (15981, [1081, 2112, 2114, 5619, 3237, 1096, 1785, 85]),\n",
       " (15962, [1019]),\n",
       " (10689, [1943, 1947, 598, 1952])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first column of inter is user\n",
    "# second column of inter is item\n",
    "\n",
    "# create a dict where keys are user and values are items\n",
    "adj_dict = {}\n",
    "for i in range(len(inter)):\n",
    "    if inter[i][0] in adj_dict:\n",
    "        adj_dict[inter[i][0]].append(inter[i][1])\n",
    "    else:\n",
    "        adj_dict[inter[i][0]] = [inter[i][1]]\n",
    "\n",
    "# show the first five items in adj_dict\n",
    "list(adj_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save adj_dict as npy file\n",
    "\n",
    "np.save(save_path+'adj_dict.npy', adj_dict, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33815934298535266\n"
     ]
    }
   ],
   "source": [
    "# count the ratio of the number of values in adj_dict where the length of values is greater than 1\n",
    "\n",
    "count = 0\n",
    "for i in adj_dict.values():\n",
    "    if len(i) > 1:\n",
    "        count += 1\n",
    "print(count/len(adj_dict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create item features (feat.npy)\n",
    "When using features, there is no need for tokenID to match inter because the index is used in features.\n",
    "\n",
    "- input\n",
    "    - Item features data in 'item_features' folder, collected and preprocessed from OpenSea\n",
    "- output\n",
    "    - An .npy formatted item features file (image, text, price, transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "image shape:  (8386, 1024)\n",
      "text shape:  (8386, 1800)\n",
      "price shape:  (8386, 64)\n",
      "transaction shape:  (8386, 64)\n",
      "\n",
      "After\n",
      "image shape:  (8386, 1024)\n",
      "text shape:  (8386, 1800)\n",
      "price shape:  (8386, 64)\n",
      "transaction shape:  (8386, 64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Only keep items that appear in inter.\n",
    "\"\"\"\n",
    "\n",
    "# print image, text, price shape\n",
    "print('Before')\n",
    "print('image shape: ', image.shape)\n",
    "print('text shape: ', text.shape)\n",
    "print('price shape: ', price.shape)\n",
    "print('transaction shape: ', transaction.shape)\n",
    "print('')\n",
    "\n",
    "item_unique = np.unique(item)\n",
    "\n",
    "# for dataset image, text, price, filter rows whose indices are in item_unique\n",
    "image = image.loc[image.index.isin(item_unique)]\n",
    "text = text.loc[text.index.isin(item_unique)]\n",
    "price = price.loc[price.index.isin(item_unique)]\n",
    "transaction = transaction.loc[transaction.index.isin(item_unique)]\n",
    "\n",
    "# print image, text, price shape\n",
    "print('After')\n",
    "print('image shape: ', image.shape)\n",
    "print('text shape: ', text.shape)\n",
    "print('price shape: ', price.shape)\n",
    "print('transaction shape: ', transaction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that the indices of image, text, price are the same, regardless of the order\n",
    "assert np.array_equal(np.sort(image.index.values), np.sort(text.index.values))\n",
    "assert np.array_equal(np.sort(image.index.values), np.sort(price.index.values))\n",
    "assert np.array_equal(np.sort(image.index.values), np.sort(transaction.index.values))\n",
    "\n",
    "# save df as npy file\n",
    "np.save(save_path+'image_feat.npy', image)\n",
    "np.save(save_path+'text_feat.npy', text)\n",
    "np.save(save_path+'price_feat.npy', price)\n",
    "np.save(save_path+'transaction_feat.npy', transaction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecBole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "392ba86a58589ad9d3867145c86eecd11f6e0889a5aad62cbef3708cb822e1d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
