{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "select NFT collection name for preprocessing, from ['azuki', 'bayc', 'coolcats', 'doodles', 'meebits']\n",
    "'''\n",
    "\n",
    "COLLECTION = 'azuki'\n",
    "\n",
    "# set save_path if not exist\n",
    "save_path = 'dataset/collections/'+COLLECTION+'/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "df_azuki = pd.read_csv(f\"dataset/transactions/{COLLECTION}.csv\")\n",
    "# drop duplicated interactions (i.e., drop rows that Buyer and Token ID are identical)\n",
    "df_azuki = df_azuki.drop_duplicates(subset=['Buyer', 'Token ID'], keep='first')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create interactions ('inter')\n",
    "- input\n",
    "    - NFT transactions data in 'transactions' folder, collected from Etherscan NFT tracker (https://etherscan.io/nfttracker)\n",
    "- output\n",
    "    - An .npy formatted interaction file (user, item, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First, exclude items that were first traded after September 2022. Only items that exist in the transaction features file will be left.\n",
    "\"\"\"\n",
    "\n",
    "transaction = pd.read_csv(f'dataset/item_features/{COLLECTION}_transactions.csv', index_col=0)\n",
    "# drop rows where Token ID is not in indices of transaction\n",
    "df_azuki = df_azuki[df_azuki['Token ID'].isin(transaction.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITEM CUT 전 거래 개수: 22990\n",
      "ITEM CUT 후 거래 개수: 16401\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ITEM_CUT: Only items that have been traded three times or more will be used.\n",
    "\"\"\"\n",
    "\n",
    "# print len of df_azuki\n",
    "print(f\"ITEM CUT 전 거래 개수: {len(df_azuki)}\")\n",
    "\n",
    "# get the list of \"Token ID\" whose count is more than 3\n",
    "item_count = df_azuki['Token ID'].value_counts()\n",
    "item_count = item_count[item_count >= 3]\n",
    "item_count = item_count.index.tolist()\n",
    "\n",
    "# drop rows whose \"Token ID\" is not in item_count\n",
    "df_azuki = df_azuki[df_azuki['Token ID'].isin(item_count)]\n",
    "\n",
    "\n",
    "# print len of df_azuki\n",
    "print(f\"ITEM CUT 후 거래 개수: {len(df_azuki)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate price labels, for later use of multi-objectives training\n",
    "\"\"\"\n",
    "\n",
    "# drop rows where 'Price' does not contain '$'\n",
    "df_azuki = df_azuki[df_azuki['Price'].str.contains(\"\\$\")]\n",
    "# convert 'Price' to the value before 'ETH'\n",
    "df_azuki['Price'] = df_azuki['Price'].apply(lambda x: x.split(' ')[2][2:-1].replace(',', '').replace('.', ''))\n",
    "df_azuki['Price'] = df_azuki['Price'].astype(float)\n",
    "\n",
    "# create a new variable 'Price_diff' which is the difference between the future price and the current price \n",
    "# get price differences from the same 'Token ID'\n",
    "df_azuki['Price_diff'] = df_azuki.groupby('Token ID')['Price'].diff(-1)\n",
    "# convert rows where 'Price_diff' is NaN into 0\n",
    "df_azuki['Price_diff'] = df_azuki['Price_diff'].fillna(0)\n",
    "# put minus to Price_diff\n",
    "df_azuki['Price_diff'] = df_azuki['Price_diff'].apply(lambda x: -x)\n",
    "# convert 'Price_diff' to 1 if the value is greater than 0, otherwise 0\n",
    "df_azuki['Price_diff'] = df_azuki['Price_diff'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# create an np.array with 'Buyer'\n",
    "user = df_azuki['Buyer'].values\n",
    "item = df_azuki['Token ID'].values\n",
    "price = df_azuki['Price_diff'].values\n",
    "\n",
    "data = (user, item, price)\n",
    "\n",
    "# save data as npy file\n",
    "np.save(save_path + f'{COLLECTION}.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user length:  9380\n",
      "item length:  3930\n",
      "inter length:  16398\n"
     ]
    }
   ],
   "source": [
    "# read azuki.npy file\n",
    "azuki = np.load(save_path + COLLECTION+'.npy', allow_pickle=True)\n",
    "\n",
    "user = azuki[0]\n",
    "item = azuki[1]\n",
    "labels = azuki[2]\n",
    "\n",
    "# print user length and item length\n",
    "print('user length: ', len(set(user)))\n",
    "print('item length: ', len(set(item)))\n",
    "print('inter length: ', len(labels))\n",
    "\n",
    "# save user length and item length as a dictionary\n",
    "dict = {'num_user': len(set(user)), 'num_item': len(set(item))}\n",
    "np.save(save_path + 'num_user_item.npy', dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4336, 1430,    0],\n",
       "       [2629,  845,    1],\n",
       "       [6055, 1431,    1],\n",
       "       ...,\n",
       "       [5096, 4891,    0],\n",
       "       [5709, 6322,    0],\n",
       "       [1557, 3792,    0]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Change the user addresses to indices starting from 0.\n",
    "\"\"\"\n",
    "\n",
    "# create a dict where keys are user and values are new indices starting from 0\n",
    "user_unique = np.unique(user)\n",
    "mapping_u = {}\n",
    "for i in range(len(user_unique)):\n",
    "    mapping_u[user_unique[i]] = i\n",
    "\n",
    "# apply mapping to user\n",
    "user = np.array([mapping_u[u] for u in user])\n",
    "\n",
    "# create a 2D np.array where first columns are users and second column is items\n",
    "inter = np.array([user, item, labels]).T\n",
    "# convert inter type as int64\n",
    "inter = inter.astype(np.int64)\n",
    "inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8266,  501,    0],\n",
       "       [6559,  264,    1],\n",
       "       [9985,  502,    1],\n",
       "       ...,\n",
       "       [9026, 1904,    0],\n",
       "       [9639, 2508,    0],\n",
       "       [5487, 1484,    0]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensure that the indices for the user and item do not overlap with each other.\n",
    "\"\"\"\n",
    "\n",
    "# 1) Change the user idx: start from num_item.\n",
    "\n",
    "num_item = len(set(item))\n",
    "user += num_item\n",
    "\n",
    "inter = np.array([user, item, labels]).T\n",
    "inter = inter.astype(np.int64)\n",
    "\n",
    "# 2) Map the item idx: start from 0.\n",
    "\n",
    "# create a dict where keys are item and values are new indices starting from 0\n",
    "item_unique = np.unique(item)\n",
    "mapping_i = {}\n",
    "for i in range(len(item_unique)):\n",
    "    mapping_i[item_unique[i]] = i\n",
    "mapping_i\n",
    "\n",
    "# convert the second column of inter to new indices using mapping\n",
    "inter[:, 1] = [mapping_i[i] for i in inter[:, 1]]\n",
    "inter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create user features (user_feat.npy)\n",
    "- input\n",
    "    - User features data in 'user_features' folder, collected and preprocessed from transactions file\n",
    "- output\n",
    "    - An .npy formatted user features file ('# of transactions', 'Avg transaction price', 'avg holding period')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_user:  9380\n"
     ]
    }
   ],
   "source": [
    "# read 'user features.csv'\n",
    "df_feature = pd.read_csv('dataset/user_features/user_features.csv', index_col=0).drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "# scaling columns \"# of transactions\", \"Avg transaction price\", \"avg holding period\": MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "names = ['# of transactions', 'Avg transaction price', 'avg holding period']\n",
    "df_feature[names] = scaler.fit_transform(df_feature[names])\n",
    "\n",
    "# convert column 'Buyer' using mapping_u\n",
    "# if the value is not in mapping_u, remove the row\n",
    "df_feature['Buyer'] = df_feature['Buyer'].apply(lambda x: mapping_u[x] if x in mapping_u else np.nan)\n",
    "df_feature = df_feature.dropna()\n",
    "# convert column 'Buyer' to int\n",
    "df_feature['Buyer'] = df_feature['Buyer'].astype(int)\n",
    "print('num_user: ', len(df_feature))\n",
    "\n",
    "# set 'Buyer' as index\n",
    "df_feature = df_feature.set_index('Buyer')\n",
    "\n",
    "# save df as npy file\n",
    "np.save(save_path+'user_feat.npy', df_feature, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train data (train.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (13118, 3)\n",
      "valid shape:  (1640, 3)\n",
      "test shape:  (1640, 3)\n"
     ]
    }
   ],
   "source": [
    "# random split inter\n",
    "train, valid_and_test = train_test_split(inter, test_size=0.2, random_state=2023)\n",
    "valid, test = train_test_split(valid_and_test, test_size=0.5, random_state=2023)\n",
    "\n",
    "# print train, valid shape\n",
    "print('train shape: ', train.shape)\n",
    "print('valid shape: ', valid.shape)\n",
    "print('test shape: ', test.shape)\n",
    "\n",
    "# save inter as npy file\n",
    "np.save(save_path+'train.npy', train, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create valid data (val.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9183, [3786]),\n",
       " (9049, [512]),\n",
       " (12689, [1090, 1275]),\n",
       " (9874, [2554]),\n",
       " (4656, [1008, 580, 1921])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using valid, create a dict where keys are unique users and values are items\n",
    "\n",
    "valid_dict = {}\n",
    "for i in range(len(valid)):\n",
    "    if valid[i][0] in valid_dict:\n",
    "        valid_dict[valid[i][0]].append(valid[i][1])\n",
    "    else:\n",
    "        valid_dict[valid[i][0]] = [valid[i][1]]\n",
    "\n",
    "# show the first five items in valid_dict\n",
    "list(valid_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the item index in the order of the most traded (popular).\n",
    "\"\"\"\n",
    "\n",
    "# concat all values in valid_dict as a list\n",
    "valid_list = []\n",
    "for i in valid_dict.values():\n",
    "    valid_list += i\n",
    "\n",
    "# value count valid_list and sort values\n",
    "value_counts = pd.Series(valid_list).value_counts().sort_values(ascending=False)\n",
    "\n",
    "# extract indices of value_counts\n",
    "indices = value_counts.index\n",
    "\n",
    "# save indices as npy\n",
    "np.save(save_path+'indices_valid.npy', indices, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([9183, 3786]), list([9049, 512]), list([12689, 1090, 1275]),\n",
       "       list([9874, 2554]), list([4656, 1008, 580, 1921])], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert to the form required by the model\n",
    "e.g., 12656: [7314, 4820, 6304] -> list([12656, 7314, 4820, 6304])\n",
    "\"\"\"\n",
    "\n",
    "# Create an empty numpy array with dtype 'object'\n",
    "my_array = np.empty(len(valid_dict), dtype=object)\n",
    "\n",
    "# Assign the lists directly to the elements of the array\n",
    "for i, (key, val) in enumerate(valid_dict.items()):\n",
    "    # include key in the list\n",
    "    my_array[i] = [key] + val\n",
    "\n",
    "# show the first five items in my_array\n",
    "my_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save my_array as npy file\n",
    "\n",
    "np.save(save_path+'val.npy', my_array, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test data (test.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9829, [1878]),\n",
       " (4000, [3610]),\n",
       " (8861, [2705]),\n",
       " (11955, [3406, 2144]),\n",
       " (11642, [2236])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using test, create a dict where keys are unique users and values are items\n",
    "\n",
    "test_dict = {}\n",
    "for i in range(len(test)):\n",
    "    if test[i][0] in test_dict:\n",
    "        test_dict[test[i][0]].append(test[i][1])\n",
    "    else:\n",
    "        test_dict[test[i][0]] = [test[i][1]]\n",
    "\n",
    "# show the first five items in test_dict\n",
    "list(test_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all values in test_dict as a list\n",
    "\n",
    "test_list = []\n",
    "for i in test_dict.values():\n",
    "    test_list += i\n",
    "\n",
    "\n",
    "# value count test_list and sort values\n",
    "\n",
    "value_counts = pd.Series(test_list).value_counts().sort_values(ascending=False)\n",
    "\n",
    "# extract indices of value_counts\n",
    "\n",
    "indices = value_counts.index\n",
    "indices\n",
    "\n",
    "# save indices as npy\n",
    "\n",
    "np.save(save_path+'indices_test.npy', indices, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([9829, 1878]), list([4000, 3610]), list([8861, 2705]),\n",
       "       list([11955, 3406, 2144]), list([11642, 2236])], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty numpy array with dtype 'object'\n",
    "my_array = np.empty(len(test_dict), dtype=object)\n",
    "\n",
    "# Assign the lists directly to the elements of the array\n",
    "for i, (key, val) in enumerate(test_dict.items()):\n",
    "    # include key in the list\n",
    "    my_array[i] = [key] + val\n",
    "\n",
    "# show the first five items in my_array\n",
    "my_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래 이렇게하면 됐었는데 에러나서 버림\n",
    "\n",
    "# # convert test_dict into a 1D np.array where each element is a list\n",
    "# # a list where the first element is the key of test_dict and the value is the value of test_dict\n",
    "\n",
    "# test_dict = np.array([[k]+v for k, v in test_dict.items()])\n",
    "# test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test_dict as npy file\n",
    "\n",
    "np.save(save_path+'test.npy', my_array, allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create adjacency matrix (adj_dict.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8266, [501]),\n",
       " (6559, [264, 2373]),\n",
       " (9985, [502, 998, 1000, 2670, 1542, 845]),\n",
       " (5727, [917, 922]),\n",
       " (12692, [495])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first column of inter is user\n",
    "# second column of inter is item\n",
    "\n",
    "# create a dict where keys are user and values are items\n",
    "adj_dict = {}\n",
    "for i in range(len(inter)):\n",
    "    if inter[i][0] in adj_dict:\n",
    "        adj_dict[inter[i][0]].append(inter[i][1])\n",
    "    else:\n",
    "        adj_dict[inter[i][0]] = [inter[i][1]]\n",
    "\n",
    "# show the first five items in adj_dict\n",
    "list(adj_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save adj_dict as npy file\n",
    "\n",
    "np.save(save_path+'adj_dict.npy', adj_dict, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30575692963752665\n"
     ]
    }
   ],
   "source": [
    "# count the ratio of the number of values in adj_dict where the length of values is greater than 1\n",
    "\n",
    "count = 0\n",
    "for i in adj_dict.values():\n",
    "    if len(i) > 1:\n",
    "        count += 1\n",
    "print(count/len(adj_dict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create item features (feat.npy)\n",
    "When using features, there is no need for tokenID to match inter because the index is used in features.\n",
    "\n",
    "- input\n",
    "    - Item features data in 'item_features' folder, collected and preprocessed from OpenSea\n",
    "- output\n",
    "    - An .npy formatted item features file (image, text, price, transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (10000, 1024)\n",
      "text shape:  (10000, 1800)\n",
      "price shape:  (8480, 64)\n",
      "transaction shape:  (8480, 64)\n"
     ]
    }
   ],
   "source": [
    "# read 'bayc_image.csv' file\n",
    "# set index as first column\n",
    "\n",
    "image = pd.read_csv(f'dataset/item_features/{COLLECTION}_image.csv', index_col=0)\n",
    "text = pd.read_csv(f'dataset/item_features/{COLLECTION}_text.csv', index_col=0)\n",
    "price = pd.read_csv(f'dataset/item_features/{COLLECTION}_prices.csv', index_col=0)\n",
    "transaction = pd.read_csv(f'dataset/item_features/{COLLECTION}_transactions.csv', index_col=0)\n",
    "\n",
    "# print image, text, price shape\n",
    "print('image shape: ', image.shape)\n",
    "print('text shape: ', text.shape)\n",
    "print('price shape: ', price.shape)\n",
    "print('transaction shape: ', transaction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (3930, 1024)\n",
      "text shape:  (3930, 1800)\n",
      "price shape:  (3930, 64)\n",
      "transaction shape:  (3930, 64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Only keep items that appear in inter.\n",
    "\"\"\"\n",
    "\n",
    "item_unique = np.unique(item)\n",
    "\n",
    "# for dataset image, text, price, filter rows whose indices are in item_unique\n",
    "image = image.loc[image.index.isin(item_unique)]\n",
    "text = text.loc[text.index.isin(item_unique)]\n",
    "price = price.loc[price.index.isin(item_unique)]\n",
    "transaction = transaction.loc[transaction.index.isin(item_unique)]\n",
    "\n",
    "# print image, text, price shape\n",
    "print('image shape: ', image.shape)\n",
    "print('text shape: ', text.shape)\n",
    "print('price shape: ', price.shape)\n",
    "print('transaction shape: ', transaction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (3930, 1024)\n",
      "text shape:  (3930, 1800)\n",
      "price shape:  (3930, 64)\n",
      "transaction shape:  (3930, 64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Keep only items that exist in the transaction features data and delete the rest. \n",
    "The purpose is to delete items that were first traded after September.\n",
    "\"\"\"\n",
    "\n",
    "# drop rows whose indices are not in indices of transaction\n",
    "image = image.loc[image.index.isin(transaction.index)]\n",
    "text = text.loc[text.index.isin(transaction.index)]\n",
    "price = price.loc[price.index.isin(transaction.index)]\n",
    "\n",
    "# print image, text, price shape\n",
    "print('image shape: ', image.shape)\n",
    "print('text shape: ', text.shape)\n",
    "print('price shape: ', price.shape)\n",
    "print('transaction shape: ', transaction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (928, 1024)\n",
      "text shape:  (928, 1800)\n",
      "price shape:  (928, 64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optional: If the length of any of image, text, price is less than len(set(item)), fill it with random values.\n",
    "\"\"\"\n",
    "\n",
    "# compare indices of image and text \n",
    "# and fill empty rows with random values\n",
    "image = image.reindex(text.index)\n",
    "\n",
    "# convert rows that are nan values to random vector\n",
    "image = image.fillna(np.random.rand(1)[0])\n",
    "\n",
    "# print image, text, price shape\n",
    "print('image shape: ', image.shape)\n",
    "print('text shape: ', text.shape)\n",
    "print('price shape: ', price.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (1646, 1024)\n",
      "text shape:  (1646, 1500)\n",
      "price shape:  (1646, 64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optional: If the length of any of image, text, price is less than len(set(item)), fill it with random values.\n",
    "\"\"\"\n",
    "\n",
    "# compare indices of image and text \n",
    "# and fill empty rows with random values\n",
    "text = text.reindex(image.index)\n",
    "\n",
    "# convert rows that are nan values to random vector\n",
    "text = text.fillna(np.random.rand(1)[0])\n",
    "\n",
    "# print image, text, price shape\n",
    "print('image shape: ', image.shape)\n",
    "print('text shape: ', text.shape)\n",
    "print('price shape: ', price.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df as npy file\n",
    "\n",
    "np.save(save_path+'image_feat.npy', image)\n",
    "np.save(save_path+'text_feat.npy', text)\n",
    "np.save(save_path+'price_feat.npy', price)\n",
    "np.save(save_path+'transaction_feat.npy', transaction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecBole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "392ba86a58589ad9d3867145c86eecd11f6e0889a5aad62cbef3708cb822e1d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
